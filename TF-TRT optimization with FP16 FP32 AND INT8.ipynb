{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment_10.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ybl8Ca4R4Ydq"},"source":["# Downloading the Data from Kaggle"]},{"cell_type":"code","metadata":{"id":"JnWUjVNNPOVx","executionInfo":{"status":"ok","timestamp":1601962560653,"user_tz":-330,"elapsed":7319,"user":{"displayName":"Akash Meshram","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE3wrC3GrVLmkpJISru64Fl3F3y5zqcX6t3iH8ig=s64","userId":"05141057680937633697"}},"outputId":"77133e8b-5e44-4be3-bc28-a4c0d21a0f7a","colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["!pip uninstall -y kaggle\n","!pip install --upgrade pip\n","!pip install kaggle==1.5.6"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found existing installation: kaggle 1.5.6\n","Uninstalling kaggle-1.5.6:\n","  Successfully uninstalled kaggle-1.5.6\n","Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (20.2.3)\n","Processing /root/.cache/pip/wheels/01/3e/ff/77407ebac3ef71a79b9166a8382aecf88415a0bcbe3c095a01/kaggle-1.5.6-py3-none-any.whl\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2.23.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2020.6.20)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2.8.1)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (1.15.0)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (1.24.3)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (4.0.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (4.41.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle==1.5.6) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle==1.5.6) (2.10)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle==1.5.6) (1.3)\n","Installing collected packages: kaggle\n","Successfully installed kaggle-1.5.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RdlewuALlU0n","executionInfo":{"status":"ok","timestamp":1601962568836,"user_tz":-330,"elapsed":5192,"user":{"displayName":"Akash Meshram","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE3wrC3GrVLmkpJISru64Fl3F3y5zqcX6t3iH8ig=s64","userId":"05141057680937633697"}},"outputId":"4387282a-8f09-4dbb-d884-c1e2ef12af5d","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":55}},"source":["from google.colab import files\n","files.upload()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-af55cc23-1f6c-438b-bc13-286c0ddf1bc7\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-af55cc23-1f6c-438b-bc13-286c0ddf1bc7\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["{}"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"fnvtkMlmnAX6"},"source":["! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/\n","! chmod 600 ~/.kaggle/kaggle.json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qZ7G3Yv22X4N","executionInfo":{"status":"ok","timestamp":1601951467815,"user_tz":-330,"elapsed":38231,"user":{"displayName":"Akash Meshram","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE3wrC3GrVLmkpJISru64Fl3F3y5zqcX6t3iH8ig=s64","userId":"05141057680937633697"}},"outputId":"5f6bc29c-6a2d-4277-ff6e-5f6fcdbf3ea1","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["!kaggle competitions download -c plant-seedlings-classification"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading plant-seedlings-classification.zip to /content\n"," 99% 1.67G/1.69G [00:23<00:00, 80.1MB/s]\n","100% 1.69G/1.69G [00:23<00:00, 77.1MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vsim20JS2tWu"},"source":["! mkdir plant-seedlings-classification\n","! unzip ./plant-seedlings-classification.zip -d plant-seedlings-classification"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hs32wNef3PnO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XMlBnsgWSJmn"},"source":["# 1) Data Loading and Preparation"]},{"cell_type":"code","metadata":{"id":"ao9e4eFFSJE7"},"source":["import numpy as np\n","import pandas as pd\n","import os\n","import PIL\n","import PIL.Image\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from tensorflow import keras\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix, classification_report"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bN4K3QMxSOCh"},"source":["data_dir = './plant-seedlings-classification/'\n","train_dir = os.path.join(data_dir, 'train')\n","test_dir = os.path.join(data_dir, 'test')\n","sample_submission = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yu9AstT4bqVM"},"source":["BATCH_SIZE = 32\n","IMG_SIZE = (270, 270)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X1eCMs9i1W8g","executionInfo":{"status":"ok","timestamp":1601962599679,"user_tz":-330,"elapsed":7078,"user":{"displayName":"Akash Meshram","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE3wrC3GrVLmkpJISru64Fl3F3y5zqcX6t3iH8ig=s64","userId":"05141057680937633697"}},"outputId":"02994d07-c95f-4edf-e8cf-3c3f37d038af","colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["print(\"Train Dataset\")\n","train_dataset  = tf.keras.preprocessing.image_dataset_from_directory(\n","  train_dir,\n","  validation_split=0.2,\n","  subset=\"training\",\n","  seed=123,\n","  shuffle=True,\n","  image_size= IMG_SIZE,\n","  batch_size= BATCH_SIZE)\n","\n","print(\"Validation Dataset\")\n","validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n","  train_dir,\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  seed=123,\n","  shuffle=True,\n","  image_size= IMG_SIZE,\n","  batch_size=BATCH_SIZE)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train Dataset\n","Found 4750 files belonging to 12 classes.\n","Using 3800 files for training.\n","Validation Dataset\n","Found 4750 files belonging to 12 classes.\n","Using 950 files for validation.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5HPszXk4vtk7","executionInfo":{"status":"ok","timestamp":1601962603114,"user_tz":-330,"elapsed":829,"user":{"displayName":"Akash Meshram","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE3wrC3GrVLmkpJISru64Fl3F3y5zqcX6t3iH8ig=s64","userId":"05141057680937633697"}},"outputId":"87be5054-3d45-4127-fd37-84e8d7167778","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["val_batches = tf.data.experimental.cardinality(validation_dataset)\n","test_dataset = validation_dataset.take(val_batches // 5)\n","validation_dataset = validation_dataset.skip(val_batches // 5)\n","\n","print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n","print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of validation batches: 24\n","Number of test batches: 6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0r32f7wfwAVR","executionInfo":{"status":"ok","timestamp":1601962606647,"user_tz":-330,"elapsed":1215,"user":{"displayName":"Akash Meshram","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE3wrC3GrVLmkpJISru64Fl3F3y5zqcX6t3iH8ig=s64","userId":"05141057680937633697"}},"outputId":"903540b6-fa00-41f8-9ec0-86dfd660f1fe","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["class_names = train_dataset.class_names\n","num_classes = len(class_names)\n","print(class_names)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent', 'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WtiVVrdvwOTI"},"source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\n","\n","train_ds = train_dataset.prefetch(buffer_size=AUTOTUNE)\n","validation_ds = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n","test_ds = test_dataset.prefetch(buffer_size=AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6yLcD00tHhZE"},"source":["# 2) Data Agumentation"]},{"cell_type":"code","metadata":{"id":"n5BDU6KgwXZQ"},"source":["data_augmentation = tf.keras.Sequential([\n","  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n","  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B-s3ua0pjbQi"},"source":["# 3) Load Models"]},{"cell_type":"code","metadata":{"id":"88702CSHlMFg"},"source":["vgg_model_file = 'drive/My Drive/week/extract_VGG16'\n","inception_model_file = 'drive/My Drive/week/extract_InceptionV3'\n","resnet_model_file = 'drive/My Drive/week/extract_Resnet50'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OmDeKsLnETrJ"},"source":["vgg_model = tf.keras.models.load_model(vgg_model_file)\n","inception_model = tf.keras.models.load_model(inception_model_file)\n","resnet_model = tf.keras.models.load_model(resnet_model_file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"to_aq8nfjig7"},"source":["# 4) TF-TRT"]},{"cell_type":"code","metadata":{"id":"zdejByYXjpMD"},"source":["import time\n","from tensorflow.python.compiler.tensorrt import trt_convert as trt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z2zwLcD0p-bY"},"source":["for data in test_ds.take(1):\n","  test_images = data[0]\n","  test_labels = data[1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zlfu79aXctBI"},"source":["## Some Helper Functions"]},{"cell_type":"code","metadata":{"id":"h4Zhy-FitUlK"},"source":["def input_map_fn():\n","    for data in test_ds.take(1):\n","      test_images = data[0]\n","      yield np.array([test_images])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hGkLkgt6ptAK"},"source":["def time_trt_model(concrete_func):\n","    times = []\n","    for i in range(500):\n","        start_time = time.time()\n","        one_prediction = concrete_func(tf.constant(test_images))\n","        delta = (time.time() - start_time)\n","        times.append(delta)\n","    mean_delta = np.array(times).mean()\n","    fps = 1 / mean_delta\n","    print('\\n average(sec):{:.2f},fps:{:.2f}'.format(mean_delta, fps))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3vPv_1fnH4WK"},"source":["def trtPredict(concrete_func):\n","  for data in test_ds.take(1):\n","    test_images = data[0]\n","    test_labels = data[1]\n","\n","  output = concrete_func(tf.constant(test_images))\n","  for key in output:\n","    precited_labels = np.argmax(output[key].numpy(), axis=1)\n","\n","  accuracy = np.mean(precited_labels == test_labels)\n","  print('\\nTest accuracy:{:.2f}'.format(accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_EDffUp8cWPb"},"source":["## Functions for FP16, FP32 and INT8"]},{"cell_type":"code","metadata":{"id":"e4K4e6CNjp4g"},"source":["def wholeTRT_FP16(savefile):\n","\n","  params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt.TrtPrecisionMode.FP16)\n","\n","  converter = trt.TrtGraphConverterV2(\n","      input_saved_model_dir=savefile,\n","      conversion_params=params)\n","  converter.convert()\n","\n","  saved_model_dir_trt = savefile[20:] + '_FP16.trt'\n","\n","  converter.save(saved_model_dir_trt)\n","\n","  root = tf.saved_model.load(saved_model_dir_trt)\n","  concrete_func = root.signatures['serving_default']\n","\n","  trtPredict(concrete_func)\n","\n","  time_trt_model(concrete_func)\n","\n","  print('\\n===============================\\n')\n","\n","def wholeTRT_FP32(savefile):\n","\n","  params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt.TrtPrecisionMode.FP32)\n","\n","  converter = trt.TrtGraphConverterV2(\n","      input_saved_model_dir=savefile,\n","      conversion_params=params)\n","  converter.convert()\n","\n","  saved_model_dir_trt = savefile[20:] + '_FP32.trt'\n","\n","  converter.save(saved_model_dir_trt)\n","\n","  root = tf.saved_model.load(saved_model_dir_trt)\n","  concrete_func = root.signatures['serving_default']\n","\n","  trtPredict(concrete_func)\n","\n","  time_trt_model(concrete_func)\n","\n","  print('\\n===============================\\n')\n","\n","def wholeTRT_INT8(savefile):\n","  params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n","    precision_mode= trt.TrtPrecisionMode.INT8,\n","    use_calibration=True)\n","\n","  converter = trt.TrtGraphConverterV2(\n","      input_saved_model_dir=savefile,\n","      conversion_params=params)\n","  \n","  converter.convert(calibration_input_fn=input_map_fn)  \n","\n","  saved_model_dir_trt = savefile[20:] + '_INT8.trt'\n","\n","  converter.save(saved_model_dir_trt)\n","\n","  root = tf.saved_model.load(saved_model_dir_trt)\n","  concrete_func = root.signatures['serving_default']\n","\n","  trtPredict(concrete_func)\n","\n","  time_trt_model(concrete_func)\n","\n","  print('\\n===============================\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rY7mhEswcevW"},"source":["### 1) FP16"]},{"cell_type":"code","metadata":{"id":"GrRvF1Sxuuhp","executionInfo":{"status":"ok","timestamp":1601952666929,"user_tz":-330,"elapsed":205993,"user":{"displayName":"Akash Meshram","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE3wrC3GrVLmkpJISru64Fl3F3y5zqcX6t3iH8ig=s64","userId":"05141057680937633697"}},"outputId":"f46a82af-a2b7-402a-c249-fb106ce0b8fd","colab":{"base_uri":"https://localhost:8080/","height":527}},"source":["wholeTRT_FP16(vgg_model_file)\n","wholeTRT_FP16(inception_model_file)\n","wholeTRT_FP16(resnet_model_file)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Linked TensorRT version: (0, 0, 0)\n","INFO:tensorflow:Loaded TensorRT version: (0, 0, 0)\n","INFO:tensorflow:Assets written to: extract_VGG16_FP16.trt/assets\n","\n","Test accuracy:0.84\n","\n"," average(sec):0.09,fps:11.51\n","\n","===============================\n","\n","INFO:tensorflow:Linked TensorRT version: (0, 0, 0)\n","INFO:tensorflow:Loaded TensorRT version: (0, 0, 0)\n","INFO:tensorflow:Assets written to: extract_InceptionV3_FP16.trt/assets\n","\n","Test accuracy:0.72\n","\n"," average(sec):0.06,fps:16.54\n","\n","===============================\n","\n","INFO:tensorflow:Linked TensorRT version: (0, 0, 0)\n","INFO:tensorflow:Loaded TensorRT version: (0, 0, 0)\n","INFO:tensorflow:Assets written to: extract_Resnet50_FP16.trt/assets\n","\n","Test accuracy:0.84\n","\n"," average(sec):0.08,fps:12.88\n","\n","===============================\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"htt_nmUYcisO"},"source":["### 2) FP32"]},{"cell_type":"code","metadata":{"id":"DHCP2_8Xu3wY","executionInfo":{"status":"ok","timestamp":1601952873908,"user_tz":-330,"elapsed":412960,"user":{"displayName":"Akash Meshram","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE3wrC3GrVLmkpJISru64Fl3F3y5zqcX6t3iH8ig=s64","userId":"05141057680937633697"}},"outputId":"58761da4-fb4b-4c82-d8c4-73f4620b7714","colab":{"base_uri":"https://localhost:8080/","height":527}},"source":["wholeTRT_FP32(vgg_model_file)\n","wholeTRT_FP32(inception_model_file)\n","wholeTRT_FP32(resnet_model_file)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Linked TensorRT version: (0, 0, 0)\n","INFO:tensorflow:Loaded TensorRT version: (0, 0, 0)\n","INFO:tensorflow:Assets written to: extract_VGG16_FP32.trt/assets\n","\n","Test accuracy:0.91\n","\n"," average(sec):0.09,fps:11.52\n","\n","===============================\n","\n","INFO:tensorflow:Linked TensorRT version: (0, 0, 0)\n","INFO:tensorflow:Loaded TensorRT version: (0, 0, 0)\n","INFO:tensorflow:Assets written to: extract_InceptionV3_FP32.trt/assets\n","\n","Test accuracy:0.91\n","\n"," average(sec):0.06,fps:16.55\n","\n","===============================\n","\n","INFO:tensorflow:Linked TensorRT version: (0, 0, 0)\n","INFO:tensorflow:Loaded TensorRT version: (0, 0, 0)\n","INFO:tensorflow:Assets written to: extract_Resnet50_FP32.trt/assets\n","\n","Test accuracy:0.97\n","\n"," average(sec):0.08,fps:12.88\n","\n","===============================\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZcrS5kSjcmj5"},"source":["### 3) INT8"]},{"cell_type":"code","metadata":{"id":"KP-jwiJqu_Zb","executionInfo":{"status":"ok","timestamp":1601954609042,"user_tz":-330,"elapsed":1671317,"user":{"displayName":"Akash Meshram","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE3wrC3GrVLmkpJISru64Fl3F3y5zqcX6t3iH8ig=s64","userId":"05141057680937633697"}},"outputId":"578477c5-ce77-457a-973b-b4e69a5b8129","colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["wholeTRT_INT8(vgg_model_file)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Linked TensorRT version: (0, 0, 0)\n","INFO:tensorflow:Loaded TensorRT version: (0, 0, 0)\n","INFO:tensorflow:Assets written to: extract_VGG16_INT8.trt/assets\n","\n","Test accuracy:0.88\n","\n"," average(sec):0.09,fps:11.52\n","\n","===============================\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1W-uKHVu1SLg","executionInfo":{"status":"ok","timestamp":1601956326671,"user_tz":-330,"elapsed":1685233,"user":{"displayName":"Akash Meshram","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE3wrC3GrVLmkpJISru64Fl3F3y5zqcX6t3iH8ig=s64","userId":"05141057680937633697"}},"outputId":"106237ab-1e5f-4e1f-ec60-3fd44855c5d9","colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["wholeTRT_INT8(inception_model_file)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Linked TensorRT version: (0, 0, 0)\n","INFO:tensorflow:Loaded TensorRT version: (0, 0, 0)\n","INFO:tensorflow:Assets written to: extract_InceptionV3_INT8.trt/assets\n","\n","Test accuracy:0.88\n","\n"," average(sec):0.06,fps:16.53\n","\n","===============================\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yEvAhtRt1UZK","executionInfo":{"status":"ok","timestamp":1601958166442,"user_tz":-330,"elapsed":1691413,"user":{"displayName":"Akash Meshram","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE3wrC3GrVLmkpJISru64Fl3F3y5zqcX6t3iH8ig=s64","userId":"05141057680937633697"}},"outputId":"0d29bf5f-f3c9-426d-8722-5c6ea56acb3f","colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["wholeTRT_INT8(resnet_model_file)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Linked TensorRT version: (0, 0, 0)\n","INFO:tensorflow:Loaded TensorRT version: (0, 0, 0)\n","INFO:tensorflow:Assets written to: extract_Resnet50_INT8.trt/assets\n","\n","Test accuracy:0.81\n","\n"," average(sec):0.08,fps:12.88\n","\n","===============================\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bBjRE9pDHriK"},"source":["# 5) TensorFlow Model Optimization Toolkit\n","\n"]},{"cell_type":"code","metadata":{"id":"hbkP9-ulDu4S"},"source":["!pip install -q tensorflow-model-optimization"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5AXW0nzsFhpI"},"source":["import tempfile\n","import tensorflow_model_optimization as tfmot"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EpbfpAJdc5Uh"},"source":["## TF Lite Evaluator function"]},{"cell_type":"code","metadata":{"id":"vK5-waPCQs2Q"},"source":["def evaluate_model(interpreter):\n","  input_index = interpreter.get_input_details()[0][\"index\"]\n","  output_index = interpreter.get_output_details()[0][\"index\"]\n","\n","  for data in validation_dataset.take(1):\n","    test_images = data[0]\n","    test_labels = data[1]\n","\n","  # Run predictions on ever y image in the \"test\" dataset.\n","  prediction_digits = []\n","  for i, test_image in enumerate(test_images.numpy()):\n","    if i % 1000 == 0:\n","      print('Evaluated on {n} results so far.'.format(n=i))\n","    # Pre-processing: add batch dimension and convert to float32 to match with\n","    # the model's input data format.\n","    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n","    interpreter.set_tensor(input_index, test_image)\n","\n","    # Run inference.\n","    interpreter.invoke()\n","\n","    # Post-processing: remove batch dimension and find the digit with highest\n","    # probability.\n","    output = interpreter.tensor(output_index)\n","    digit = np.argmax(output()[0])\n","    prediction_digits.append(digit)\n","\n","  print('\\n')\n","  # Compare prediction results with ground truth labels to calculate accuracy.\n","  prediction_digits = np.array(prediction_digits)\n","  accuracy = (prediction_digits == test_labels.numpy()).mean()\n","  return accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GXmH5i2jdGwn"},"source":["### Helper function for file size"]},{"cell_type":"code","metadata":{"id":"Wc59abF9Xku8"},"source":["def get_gzipped_model_size(file):\n","  # Returns size of gzipped model, in bytes.\n","  import os\n","  import zipfile\n","\n","  _, zipped_file = tempfile.mkstemp('.zip')\n","  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n","    f.write(file)\n","\n","  return os.path.getsize(zipped_file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HNHlNjDTdAdX"},"source":["# Pruning and Quantization function"]},{"cell_type":"code","metadata":{"id":"kpl8NFJtHHjW"},"source":["def pruned_and_qunatized(basemodel):\n","  model = basemodel\n","\n","  model.compile(\n","    optimizer='RMSprop',\n","    loss='sparse_categorical_crossentropy',\n","    metrics=['accuracy'])\n","\n","  \"\"\"\n","  Base Model\n","  \"\"\"\n","\n","  _, baseline_model_accuracy = model.evaluate(test_ds, verbose=0)  \n","  print('Baseline test accuracy:', baseline_model_accuracy)\n","\n","  _, keras_file = tempfile.mkstemp('.h5')\n","  tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n","  print('Saved baseline model to:', keras_file)\n","\n","  \"\"\"\n","  Pruning Model\n","  \"\"\"\n","  \n","  model_for_export = tfmot.sparsity.keras.strip_pruning(model)\n","\n","  _, pruned_keras_file = tempfile.mkstemp('.h5')\n","  tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n","  print('Saved pruned Keras model to:', pruned_keras_file)\n","\n","\n","  print(\"\\nSize of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n","  print(\"Size of gzipped pruned Keras model: %.2f bytes\\n\" % (get_gzipped_model_size(pruned_keras_file)))\n","\n","  \"\"\"\n","  Qunatized Model\n","  \"\"\"\n","\n","  converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n","  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","  quantized_and_pruned_tflite_model = converter.convert()\n","\n","  _, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite')\n","\n","  with open(quantized_and_pruned_tflite_file, 'wb') as f:\n","    f.write(quantized_and_pruned_tflite_model)\n","\n","  print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n","\n","  print(\"\\nSize of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n","  print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\\n\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))\n","\n","\n","  interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_tflite_model)\n","  interpreter.allocate_tensors()\n","\n","  test_accuracy = evaluate_model(interpreter)\n","\n","  print('Pruned and quantized test_accuracy:', test_accuracy)\n","  print('Baseline test accuracy:', baseline_model_accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oRenCHugdLhP"},"source":["## 1) VGG16"]},{"cell_type":"code","metadata":{"id":"JajHPEnMixH8","executionInfo":{"status":"ok","timestamp":1601962815741,"user_tz":-330,"elapsed":79328,"user":{"displayName":"Akash Meshram","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE3wrC3GrVLmkpJISru64Fl3F3y5zqcX6t3iH8ig=s64","userId":"05141057680937633697"}},"outputId":"e4729948-e831-443a-c499-83d5de93435c","colab":{"base_uri":"https://localhost:8080/","height":445}},"source":["pruned_and_qunatized(vgg_model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Baseline test accuracy: 0.8645833134651184\n","Saved baseline model to: /tmp/tmp4z1zpl_t.h5\n","Saved pruned Keras model to: /tmp/tmp_tvvej1i.h5\n","\n","Size of gzipped baseline Keras model: 55227288.00 bytes\n","Size of gzipped pruned Keras model: 55227288.00 bytes\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","INFO:tensorflow:Assets written to: /tmp/tmpjjk5gge5/assets\n","Saved quantized and pruned TFLite model to: /tmp/tmpiq0vztgr.tflite\n","\n","Size of gzipped baseline Keras model: 55227288.00 bytes\n","Size of gzipped pruned and quantized TFlite model: 8636002.00 bytes\n","\n","Evaluated on 0 results so far.\n","\n","\n","Pruned and quantized test_accuracy: 0.90625\n","Baseline test accuracy: 0.8645833134651184\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U6lw6krQdaiB"},"source":["## 2) INCEPTION V4"]},{"cell_type":"code","metadata":{"id":"umjmCk62ZGrU","executionInfo":{"status":"ok","timestamp":1601962927574,"user_tz":-330,"elapsed":76949,"user":{"displayName":"Akash Meshram","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE3wrC3GrVLmkpJISru64Fl3F3y5zqcX6t3iH8ig=s64","userId":"05141057680937633697"}},"outputId":"966c67bc-c0c7-4275-bc44-98522d169ab6","colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["pruned_and_qunatized(inception_model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Baseline test accuracy: 0.8229166865348816\n","Saved baseline model to: /tmp/tmpyjg3yicy.h5\n","Saved pruned Keras model to: /tmp/tmp2n8p2p74.h5\n","\n","Size of gzipped baseline Keras model: 82962682.00 bytes\n","Size of gzipped pruned Keras model: 82962678.00 bytes\n","\n","INFO:tensorflow:Assets written to: /tmp/tmp6vlexxw8/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /tmp/tmp6vlexxw8/assets\n"],"name":"stderr"},{"output_type":"stream","text":["Saved quantized and pruned TFLite model to: /tmp/tmpxmn_1jo2.tflite\n","\n","Size of gzipped baseline Keras model: 82962682.00 bytes\n","Size of gzipped pruned and quantized TFlite model: 15370417.00 bytes\n","\n","Evaluated on 0 results so far.\n","\n","\n","Pruned and quantized test_accuracy: 0.75\n","Baseline test accuracy: 0.8229166865348816\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d6Hr-Ba_dQMH"},"source":["##3) RESNET50"]},{"cell_type":"code","metadata":{"id":"VV2IgTIpagdl","executionInfo":{"status":"ok","timestamp":1601962997258,"user_tz":-330,"elapsed":145456,"user":{"displayName":"Akash Meshram","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE3wrC3GrVLmkpJISru64Fl3F3y5zqcX6t3iH8ig=s64","userId":"05141057680937633697"}},"outputId":"6e5644a3-4cc0-4e84-eb6c-90b4ac17f0f6","colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["pruned_and_qunatized(resnet_model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Baseline test accuracy: 0.8802083134651184\n","Saved baseline model to: /tmp/tmpa_si6wjx.h5\n","Saved pruned Keras model to: /tmp/tmp6thalmbq.h5\n","\n","Size of gzipped baseline Keras model: 89472862.00 bytes\n","Size of gzipped pruned Keras model: 89472859.00 bytes\n","\n","INFO:tensorflow:Assets written to: /tmp/tmpy9m15v0o/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /tmp/tmpy9m15v0o/assets\n"],"name":"stderr"},{"output_type":"stream","text":["Saved quantized and pruned TFLite model to: /tmp/tmpo8v8aszr.tflite\n","\n","Size of gzipped baseline Keras model: 89472862.00 bytes\n","Size of gzipped pruned and quantized TFlite model: 15317161.00 bytes\n","\n","Evaluated on 0 results so far.\n","\n","\n","Pruned and quantized test_accuracy: 1.0\n","Baseline test accuracy: 0.8802083134651184\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"763NfCVgUJoW"},"source":[""],"execution_count":null,"outputs":[]}]}
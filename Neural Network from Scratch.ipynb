{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Neural Network.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"FwwqRjkyfUcY","colab_type":"code","colab":{}},"source":["import numpy as np\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"neAbxV4EfUcb","colab_type":"code","colab":{}},"source":["from tensorflow.keras.datasets import mnist\n","(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6IHZHg2QfUcd","colab_type":"code","colab":{},"outputId":"72605ffe-04a1-433c-f027-4b106ff0ac3a"},"source":["print('MNIST Dataset Shape:')\n","print('X_train: ' + str(X_train.shape))\n","print('Y_train: ' + str(Y_train.shape))\n","print('X_test:  '  + str(X_test.shape))\n","print('Y_test:  '  + str(Y_test.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MNIST Dataset Shape:\n","X_train: (60000, 28, 28)\n","Y_train: (60000,)\n","X_test:  (10000, 28, 28)\n","Y_test:  (10000,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1-BuzaHwfUcg","colab_type":"code","colab":{}},"source":["def data_preprocess():\n","    training_inputs = [np.reshape(x/256, (784, 1)) for x in X_train[:50000]]\n","    train_data = zip(training_inputs, Y_train[:50000])\n","    \n","    valid_inputs = [np.reshape(x/256, (784, 1)) for x in X_train[50000:]]\n","    valid_data = zip(valid_inputs, Y_train[50000:])\n","    \n","    test_inputs = [np.reshape(x/256, (784, 1)) for x in X_test]\n","    test_data = zip(test_inputs, Y_test)\n","    \n","    return (train_data, valid_data, test_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ICRNEHv4fUch","colab_type":"code","colab":{}},"source":["def sigmoid(z):\n","    return 1.0/(1.0+np.exp(-z))\n","\n","def d_sigmoid(z):\n","    return sigmoid(z)*(1-sigmoid(z))\n","\n","def rmse(x, y):\n","    return ((x-y)**2)/2\n","\n","def d_rmse(x, y):\n","    return (x-y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8YjiPSZCfUck","colab_type":"code","colab":{}},"source":["class Network:\n","    def __init__ (self, hiddenLayerSize = 10):\n","        \"\"\"Creating weights and biases for two layers, one hidden and one output\"\"\"\n","        self.weights = [np.random.randn(hiddenLayerSize, 784) , np.random.randn(10, hiddenLayerSize)]\n","        self.biases = [np.random.randn(hiddenLayerSize, 1) , np.random.randn(10, 1)]\n","        \n","    def predict(self, activation):\n","        \"\"\"Predicting for single input\"\"\"\n","        activation = sigmoid(np.dot(self.weights[0], activation) + self.biases[0])\n","        activation = sigmoid(np.dot(self.weights[1], activation) + self.biases[1])\n","        return activation\n","    \n","    def fit(self, train_data, epochs, batch_size, learning_rate, valid_data=None):\n","        train_data = list(train_data)\n","        nTrain = len(train_data)\n","        \n","        if valid_data:\n","            valid_data = list(valid_data)\n","            nValid = len(valid_data)\n","        \n","        # Training\n","        for e in range(epochs):\n","            random.shuffle(train_data)\n","            batch_data = [train_data[k:k+batch_size] for k in range(0, nTrain, batch_size)]\n","            for batch in batch_data:\n","                self.gda(batch, learning_rate)\n","            if valid_data:\n","                print(\"Epoch {:3} - loss {:7.4f} - accuracy {:7.4f}\".format(e + 1, self.loss(valid_data), self.evaluate(valid_data) / nValid));\n","            else:\n","                print(\"Epoch {} complete\".format(e + 1))\n","            \n","    def gda(self, batch, learning_rate):\n","        sum_db = [np.zeros(b.shape) for b in self.biases]\n","        sum_dw = [np.zeros(w.shape) for w in self.weights]\n","        \n","        for X, y in batch:\n","            db, dw = self.backpropogation(X, y)\n","            sum_db = [b + sb for b, sb in zip(sum_db, db)]\n","            sum_dw = [w + sw for w, sw in zip(sum_dw, dw)]\n","        \n","        self.biases = [b-(learning_rate/len(batch))*sb for b, sb in zip(self.biases, sum_db)]\n","        self.weights = [w-(learning_rate/len(batch))*sw for w, sw in zip(self.weights, sum_dw)]\n","        \n","        \n","    def backpropogation(self, X, y):        \n","        db = [np.zeros(b.shape) for b in self.biases]\n","        dw = [np.zeros(w.shape) for w in self.weights]\n","        \n","        # Coverting output to one hot vector\n","        Y = np.zeros((10, 1))\n","        Y[y] = 1\n","                \n","        # Feedforward\n","        \"\"\"Layer 1\"\"\"\n","        weighted_input_1 = np.dot(self.weights[0], X) + self.biases[0] \n","        activation_1 = sigmoid(weighted_input_1)\n","        \"\"\"Layer 2\"\"\"\n","        weighted_input_2 = np.dot(self.weights[1], activation_1) + self.biases[1]\n","        activation_2 = sigmoid(weighted_input_2)\n","        \n","        # Backwordpass\n","        \"\"\"Layer 2\"\"\"\n","        error = d_rmse(activation_2, Y) * d_sigmoid(weighted_input_2)\n","        db[1] = error\n","        dw[1] = np.dot(error, activation_1.transpose())\n","        \"\"\"Layer 1\"\"\"\n","        error = np.dot(self.weights[1].transpose(), error) * d_sigmoid(weighted_input_1)\n","        db[0] = error\n","        dw[0] = np.dot(error, X.transpose())\n","        \n","        return (db, dw)\n","    \n","    def evaluate(self, data):\n","        results = [(np.argmax(self.predict(x)), y) for (x, y) in data]\n","        return sum(int(x == y) for (x, y) in results)\n","    \n","    def loss(self, data):\n","        results = [rmse(np.argmax(self.predict(x)), y) for (x, y) in data]\n","        return sum(results)/len(data)\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"um6iRC5rfUcm","colab_type":"code","colab":{},"outputId":"f417c1bc-d58b-415d-9913-5829eec043a8"},"source":["training_data, valid_data, test_data = data_preprocess()\n","nn = Network(20)\n","nn.fit(training_data, 20, 10, 3.0, valid_data=valid_data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch   1 - loss  0.8186 - accuracy  0.9032\n","Epoch   2 - loss  0.6313 - accuracy  0.9231\n","Epoch   3 - loss  0.5900 - accuracy  0.9302\n","Epoch   4 - loss  0.6171 - accuracy  0.9258\n","Epoch   5 - loss  0.5842 - accuracy  0.9339\n","Epoch   6 - loss  0.5570 - accuracy  0.9355\n","Epoch   7 - loss  0.5361 - accuracy  0.9391\n","Epoch   8 - loss  0.5765 - accuracy  0.9352\n","Epoch   9 - loss  0.5091 - accuracy  0.9392\n","Epoch  10 - loss  0.5150 - accuracy  0.9395\n","Epoch  11 - loss  0.5270 - accuracy  0.9393\n","Epoch  12 - loss  0.5315 - accuracy  0.9374\n","Epoch  13 - loss  0.4647 - accuracy  0.9439\n","Epoch  14 - loss  0.4912 - accuracy  0.9424\n","Epoch  15 - loss  0.5331 - accuracy  0.9414\n","Epoch  16 - loss  0.4786 - accuracy  0.9443\n","Epoch  17 - loss  0.5124 - accuracy  0.9415\n","Epoch  18 - loss  0.5125 - accuracy  0.9404\n","Epoch  19 - loss  0.4824 - accuracy  0.9456\n","Epoch  20 - loss  0.4994 - accuracy  0.9447\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5eT-ut5WfUco","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}